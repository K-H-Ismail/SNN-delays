{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Device = cpu\n"
     ]
    }
   ],
   "source": [
    "from datasets import SHD_dataloaders, SSC_dataloaders\n",
    "from config import Config\n",
    "from ann import ANN\n",
    "from snn_delays import SnnDelays\n",
    "import torch\n",
    "\n",
    "device = 'cpu'#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n===> Device = {device}\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "PATH = f'../{config.wandb_run_name}.pt' #f'/content/drive/MyDrive/{config.wandb_run_name}.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [../Datasets/SSC/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/SSC/extract] manually, then SpikingJelly will re-extract files from [../Datasets/SSC/download].\n",
      "The directory [../Datasets/SSC/duration_20] already exists.\n",
      "The directory [../Datasets/SSC/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/SSC/extract] manually, then SpikingJelly will re-extract files from [../Datasets/SSC/download].\n",
      "The directory [../Datasets/SSC/duration_20] already exists.\n",
      "The directory [../Datasets/SSC/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/SSC/extract] manually, then SpikingJelly will re-extract files from [../Datasets/SSC/download].\n",
      "The directory [../Datasets/SSC/duration_20] already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthvnvtos\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/thanatos/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thanatos/PhD/snn-delays/SNN-delays/wandb/run-20230418_134942-tif44ti0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thvnvtos/Models%20comparison/runs/tif44ti0' target=\"_blank\">(Pre-train)TestingSSC||seed=0||snn_delays||ssc||20ms||bins=10</a></strong> to <a href='https://wandb.ai/thvnvtos/Models%20comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thvnvtos/Models%20comparison' target=\"_blank\">https://wandb.ai/thvnvtos/Models%20comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thvnvtos/Models%20comparison/runs/tif44ti0' target=\"_blank\">https://wandb.ai/thvnvtos/Models%20comparison/runs/tif44ti0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m train_loader, valid_loader, test_loader \u001b[39m=\u001b[39m SSC_dataloaders(config)\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m SnnDelays(config)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(train_loader, valid_loader, device)\n\u001b[1;32m      7\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), PATH)\n",
      "File \u001b[0;32m~/PhD/snn-delays/SNN-delays/model.py:151\u001b[0m, in \u001b[0;36mModel.train_model\u001b[0;34m(self, train_loader, valid_loader, device)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m#last element in the tuple corresponds to the collate_fn return\u001b[39;00m\n\u001b[1;32m    150\u001b[0m loss_batch, metric_batch \u001b[39m=\u001b[39m [], []\n\u001b[0;32m--> 151\u001b[0m \u001b[39mfor\u001b[39;00m i, (x, y, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m    152\u001b[0m     \u001b[39m# x for shd and ssc is: (batch, neurons, time)\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    154\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/spikingjelly-0.0.0.0.14-py3.10.egg/spikingjelly/datasets/shd.py:806\u001b[0m, in \u001b[0;36mSpikingSpeechCommands.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[39mreturn\u001b[39;00m events, label\n\u001b[1;32m    805\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 806\u001b[0m     frames \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframes_path[i], allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m'\u001b[39;49m\u001b[39mframes\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    807\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframes_label[i]\n\u001b[1;32m    809\u001b[0m     binned_len \u001b[39m=\u001b[39m frames\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_bins\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/numpy/lib/npyio.py:253\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    252\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(\u001b[39mbytes\u001b[39;49m,\n\u001b[1;32m    254\u001b[0m                              allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallow_pickle,\n\u001b[1;32m    255\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    256\u001b[0m                              max_header_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_header_size)\n\u001b[1;32m    257\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/numpy/lib/format.py:765\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    763\u001b[0m version \u001b[39m=\u001b[39m read_magic(fp)\n\u001b[1;32m    764\u001b[0m _check_version(version)\n\u001b[0;32m--> 765\u001b[0m shape, fortran_order, dtype \u001b[39m=\u001b[39m _read_array_header(\n\u001b[1;32m    766\u001b[0m         fp, version, max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    767\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    768\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/numpy/lib/format.py:616\u001b[0m, in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version, max_header_size)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[39m# The header is a pretty-printed string representation of a literal\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[39m# Python dictionary with trailing newlines padded to a ARRAY_ALIGN byte\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[39m# boundary. The keys are strings.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[39m# Versions (2, 0) and (1, 0) could have been created by a Python 2\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[39m# implementation before header filtering was implemented.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[39mif\u001b[39;00m version \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m):\n\u001b[0;32m--> 616\u001b[0m     header \u001b[39m=\u001b[39m _filter_header(header)\n\u001b[1;32m    617\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     d \u001b[39m=\u001b[39m safe_eval(header)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/numpy/lib/format.py:579\u001b[0m, in \u001b[0;36m_filter_header\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    577\u001b[0m         tokens\u001b[39m.\u001b[39mappend(token)\n\u001b[1;32m    578\u001b[0m     last_token_was_number \u001b[39m=\u001b[39m (token_type \u001b[39m==\u001b[39m tokenize\u001b[39m.\u001b[39mNUMBER)\n\u001b[0;32m--> 579\u001b[0m \u001b[39mreturn\u001b[39;00m tokenize\u001b[39m.\u001b[39;49muntokenize(tokens)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/tokenize.py:280\u001b[0m, in \u001b[0;36muntokenize\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transform tokens back into Python source code.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39mIt returns a bytes object, encoded using the ENCODING\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39mtoken, which is the first token sequence output by tokenize.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39m    assert t1 == t2\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    279\u001b[0m ut \u001b[39m=\u001b[39m Untokenizer()\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m ut\u001b[39m.\u001b[39;49muntokenize(iterable)\n\u001b[1;32m    281\u001b[0m \u001b[39mif\u001b[39;00m ut\u001b[39m.\u001b[39mencoding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mencode(ut\u001b[39m.\u001b[39mencoding)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/tokenize.py:194\u001b[0m, in \u001b[0;36mUntokenizer.untokenize\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    193\u001b[0m tok_type, token, start, end, line \u001b[39m=\u001b[39m t\n\u001b[0;32m--> 194\u001b[0m \u001b[39mif\u001b[39;00m tok_type \u001b[39m==\u001b[39m ENCODING:\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding \u001b[39m=\u001b[39m token\n\u001b[1;32m    196\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train_loader, valid_loader = SHD_dataloaders(config)\n",
    "train_loader, valid_loader, test_loader = SSC_dataloaders(config)\n",
    "\n",
    "model = SnnDelays(config).to(device)\n",
    "\n",
    "model.train_model(train_loader, valid_loader, device)\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [../Datasets/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/extract] manually, then SpikingJelly will re-extract files from [../Datasets/download].\n",
      "The directory [../Datasets/duration_10] already exists.\n",
      "The directory [../Datasets/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/extract] manually, then SpikingJelly will re-extract files from [../Datasets/download].\n",
      "The directory [../Datasets/duration_10] already exists.\n",
      "tensor(-4., device='cuda:0')\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.7098e-07, -4.7809e-01, -1.7098e-07,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "config.DCLSversion = 'v2'\n",
    "config.model_type = 'snn_delays_lr0'\n",
    "config.wandb_run_name = config.wandb_run_name.replace('(Pre-train)', '(Fine-tune_BIGLR_PLIF)')\n",
    "config.wandb_group_name = config.wandb_group_name.replace('(Pre-train)', '(Fine-tune)')\n",
    "\n",
    "config.lr_w = 1e-3\n",
    "config.max_lr_w = 2 * config.lr_w\n",
    "\n",
    "#config.dropout_p = 0.1\n",
    "#config.stateful_synapse_learnable = True\n",
    "#config.spiking_neuron_type = 'plif'\n",
    "config.epochs = 50\n",
    "\n",
    "train_loader, valid_loader = SHD_dataloaders(config)\n",
    "model_d = SnnDelays(config).to(device)\n",
    "\n",
    "\n",
    "model_d.load_state_dict(torch.load(PATH), strict=False)\n",
    "\n",
    "model_d.round_pos()\n",
    "\n",
    "w = model_d.blocks[0][0][0].weight\n",
    "p = model_d.blocks[0][0][0].P\n",
    "sig = model_d.blocks[0][0][0].SIG\n",
    "\n",
    "kernel = model_d.blocks[0][0][0].GCK.forward(w, p , sig)\n",
    "print(p[0,1,0,0])\n",
    "print(kernel[1,0])\n",
    "\n",
    "print(sig[0,0,0,0])\n",
    "\n",
    "model_d.train_model(train_loader, valid_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7., device='cuda:0')\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -2.5519,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "w = model_d.blocks[0][0][0].weight\n",
    "p = model_d.blocks[0][0][0].P\n",
    "sig = model_d.blocks[0][0][0].SIG\n",
    "\n",
    "kernel = model_d.blocks[0][0][0].GCK.forward(w, p , sig)\n",
    "print(p[0,2,0,0])\n",
    "print(kernel[2,0])\n",
    "\n",
    "print(sig[0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn-delays",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
