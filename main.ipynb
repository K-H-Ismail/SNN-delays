{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Device = cuda\n"
     ]
    }
   ],
   "source": [
    "from datasets import SHD_dataloaders, SSC_dataloaders\n",
    "from config import Config\n",
    "from ann import ANN\n",
    "from snn_delays import SnnDelays\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n===> Device = {device}\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "model = SnnDelays(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [../Datasets/SHD/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/SHD/extract] manually, then SpikingJelly will re-extract files from [../Datasets/SHD/download].\n",
      "The directory [../Datasets/SHD/duration_10] already exists.\n",
      "The directory [../Datasets/SHD/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/SHD/extract] manually, then SpikingJelly will re-extract files from [../Datasets/SHD/download].\n",
      "The directory [../Datasets/SHD/duration_10] already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthvnvtos\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/thanatos/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thanatos/PhD/snn-delays/SNN-delays/wandb/run-20230420_195711-kw892mgx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thvnvtos/Models%20comparison/runs/kw892mgx' target=\"_blank\">(Pre-train)Testing_code_SOTA||seed=0||snn_delays||shd||10ms||bins=5</a></strong> to <a href='https://wandb.ai/thvnvtos/Models%20comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thvnvtos/Models%20comparison' target=\"_blank\">https://wandb.ai/thvnvtos/Models%20comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thvnvtos/Models%20comparison/runs/kw892mgx' target=\"_blank\">https://wandb.ai/thvnvtos/Models%20comparison/runs/kw892mgx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> Epoch 0 : \n",
      "Loss Train = 5.322  |  Acc Train = 6.31% \n",
      "Loss Valid = 2.923  |  Acc Valid = 12.91%\n",
      "Loss Test  = 2.831  |  Acc Test = 16.76%\n",
      "=====> Epoch 1 : \n",
      "Loss Train = 3.747  |  Acc Train = 11.51% \n",
      "Loss Valid = 2.315  |  Acc Valid = 32.34%\n",
      "Loss Test  = 2.258  |  Acc Test = 32.72%\n",
      "=====> Epoch 2 : \n",
      "Loss Train = 2.998  |  Acc Train = 19.91% \n",
      "Loss Valid = 1.793  |  Acc Valid = 46.45%\n",
      "Loss Test  = 1.795  |  Acc Test = 47.66%\n",
      "=====> Epoch 3 : \n",
      "Loss Train = 2.432  |  Acc Train = 31.08% \n",
      "Loss Valid = 1.322  |  Acc Valid = 62.65%\n",
      "Loss Test  = 1.381  |  Acc Test = 61.24%\n",
      "=====> Epoch 4 : \n",
      "Loss Train = 1.938  |  Acc Train = 41.43% \n",
      "Loss Valid = 0.929  |  Acc Valid = 74.38%\n",
      "Loss Test  = 1.064  |  Acc Test = 68.53%\n",
      "=====> Epoch 5 : \n",
      "Loss Train = 1.508  |  Acc Train = 53.03% \n",
      "Loss Valid = 0.658  |  Acc Valid = 81.94%\n",
      "Loss Test  = 0.800  |  Acc Test = 73.78%\n",
      "=====> Epoch 6 : \n",
      "Loss Train = 1.191  |  Acc Train = 62.12% \n",
      "Loss Valid = 0.482  |  Acc Valid = 86.49%\n",
      "Loss Test  = 0.681  |  Acc Test = 77.57%\n",
      "=====> Epoch 7 : \n",
      "Loss Train = 0.902  |  Acc Train = 70.96% \n",
      "Loss Valid = 0.343  |  Acc Valid = 90.12%\n",
      "Loss Test  = 0.551  |  Acc Test = 81.56%\n",
      "=====> Epoch 8 : \n",
      "Loss Train = 0.752  |  Acc Train = 75.45% \n",
      "Loss Valid = 0.288  |  Acc Valid = 91.50%\n",
      "Loss Test  = 0.512  |  Acc Test = 80.57%\n",
      "=====> Epoch 9 : \n",
      "Loss Train = 0.602  |  Acc Train = 80.92% \n",
      "Loss Valid = 0.238  |  Acc Valid = 92.58%\n",
      "Loss Test  = 0.406  |  Acc Test = 85.67%\n",
      "=====> Epoch 10 : \n",
      "Loss Train = 0.520  |  Acc Train = 82.74% \n",
      "Loss Valid = 0.160  |  Acc Valid = 95.55%\n",
      "Loss Test  = 0.329  |  Acc Test = 89.27%\n",
      "=====> Epoch 11 : \n",
      "Loss Train = 0.409  |  Acc Train = 86.49% \n",
      "Loss Valid = 0.143  |  Acc Valid = 95.57%\n",
      "Loss Test  = 0.376  |  Acc Test = 86.38%\n",
      "=====> Epoch 12 : \n",
      "Loss Train = 0.361  |  Acc Train = 88.46% \n",
      "Loss Valid = 0.116  |  Acc Valid = 97.31%\n",
      "Loss Test  = 0.295  |  Acc Test = 89.20%\n",
      "=====> Epoch 13 : \n",
      "Loss Train = 0.323  |  Acc Train = 89.55% \n",
      "Loss Valid = 0.122  |  Acc Valid = 96.85%\n",
      "Loss Test  = 0.317  |  Acc Test = 89.26%\n",
      "=====> Epoch 14 : \n",
      "Loss Train = 0.281  |  Acc Train = 91.14% \n",
      "Loss Valid = 0.107  |  Acc Valid = 97.01%\n",
      "Loss Test  = 0.319  |  Acc Test = 88.98%\n",
      "=====> Epoch 15 : \n",
      "Loss Train = 0.248  |  Acc Train = 92.23% \n",
      "Loss Valid = 0.087  |  Acc Valid = 97.52%\n",
      "Loss Test  = 0.302  |  Acc Test = 89.25%\n",
      "=====> Epoch 16 : \n",
      "Loss Train = 0.221  |  Acc Train = 92.95% \n",
      "Loss Valid = 0.074  |  Acc Valid = 98.12%\n",
      "Loss Test  = 0.329  |  Acc Test = 89.13%\n",
      "=====> Epoch 17 : \n",
      "Loss Train = 0.203  |  Acc Train = 93.59% \n",
      "Loss Valid = 0.085  |  Acc Valid = 97.77%\n",
      "Loss Test  = 0.337  |  Acc Test = 88.08%\n",
      "=====> Epoch 18 : \n",
      "Loss Train = 0.191  |  Acc Train = 93.66% \n",
      "Loss Valid = 0.080  |  Acc Valid = 97.70%\n",
      "Loss Test  = 0.326  |  Acc Test = 88.96%\n",
      "=====> Epoch 19 : \n",
      "Loss Train = 0.172  |  Acc Train = 94.56% \n",
      "Loss Valid = 0.060  |  Acc Valid = 98.54%\n",
      "Loss Test  = 0.237  |  Acc Test = 92.18%\n",
      "=====> Epoch 20 : \n",
      "Loss Train = 0.165  |  Acc Train = 94.65% \n",
      "Loss Valid = 0.071  |  Acc Valid = 97.85%\n",
      "Loss Test  = 0.300  |  Acc Test = 89.09%\n",
      "=====> Epoch 21 : \n",
      "Loss Train = 0.167  |  Acc Train = 94.82% \n",
      "Loss Valid = 0.066  |  Acc Valid = 98.30%\n",
      "Loss Test  = 0.296  |  Acc Test = 89.52%\n",
      "=====> Epoch 22 : \n",
      "Loss Train = 0.158  |  Acc Train = 95.17% \n",
      "Loss Valid = 0.060  |  Acc Valid = 98.52%\n",
      "Loss Test  = 0.239  |  Acc Test = 91.74%\n",
      "=====> Epoch 23 : \n",
      "Loss Train = 0.152  |  Acc Train = 95.06% \n",
      "Loss Valid = 0.068  |  Acc Valid = 98.24%\n",
      "Loss Test  = 0.254  |  Acc Test = 91.19%\n",
      "=====> Epoch 24 : \n",
      "Loss Train = 0.158  |  Acc Train = 95.34% \n",
      "Loss Valid = 0.067  |  Acc Valid = 98.24%\n",
      "Loss Test  = 0.234  |  Acc Test = 91.83%\n",
      "=====> Epoch 25 : \n",
      "Loss Train = 0.153  |  Acc Train = 95.16% \n",
      "Loss Valid = 0.074  |  Acc Valid = 98.18%\n",
      "Loss Test  = 0.303  |  Acc Test = 89.39%\n",
      "=====> Epoch 26 : \n",
      "Loss Train = 0.154  |  Acc Train = 95.19% \n",
      "Loss Valid = 0.072  |  Acc Valid = 98.36%\n",
      "Loss Test  = 0.294  |  Acc Test = 90.49%\n",
      "=====> Epoch 27 : \n",
      "Loss Train = 0.153  |  Acc Train = 95.19% \n",
      "Loss Valid = 0.064  |  Acc Valid = 98.70%\n",
      "Loss Test  = 0.255  |  Acc Test = 90.64%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_loader, valid_loader, test_loader \u001b[39m=\u001b[39m SHD_dataloaders(config)\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(train_loader, valid_loader, test_loader, device)\n",
      "File \u001b[0;32m~/PhD/snn-delays/SNN-delays/model.py:183\u001b[0m, in \u001b[0;36mModel.train_model\u001b[0;34m(self, train_loader, valid_loader, test_loader, device)\u001b[0m\n\u001b[1;32m    179\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    181\u001b[0m \u001b[39mfor\u001b[39;00m opt \u001b[39min\u001b[39;00m optimizers: opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 183\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m    184\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalc_loss(output, y)\n\u001b[1;32m    186\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/PhD/snn-delays/SNN-delays/snn_delays.py:206\u001b[0m, in \u001b[0;36mSnnDelays.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39m# permute out: (batch, neurons, time) => (time, batch, neurons)  For final spiking neuron filter\u001b[39;00m\n\u001b[1;32m    205\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][\u001b[39m1\u001b[39;49m][\u001b[39m0\u001b[39;49m](out)\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mloss \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mspike_count\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    209\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mv_seq\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/spikingjelly-0.0.0.0.14-py3.10.egg/spikingjelly/activation_based/base.py:270\u001b[0m, in \u001b[0;36mMemoryModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingle_step_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    269\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mm\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_step_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    271\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/spikingjelly-0.0.0.0.14-py3.10.egg/spikingjelly/activation_based/neuron.py:933\u001b[0m, in \u001b[0;36mLIFNode.multi_step_forward\u001b[0;34m(self, x_seq)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 933\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mmulti_step_forward(x_seq)\n\u001b[1;32m    934\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcupy\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    936\u001b[0m         hard_reset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_reset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/spikingjelly-0.0.0.0.14-py3.10.egg/spikingjelly/activation_based/neuron.py:250\u001b[0m, in \u001b[0;36mBaseNode.multi_step_forward\u001b[0;34m(self, x_seq)\u001b[0m\n\u001b[1;32m    248\u001b[0m     v_seq \u001b[39m=\u001b[39m []\n\u001b[1;32m    249\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T):\n\u001b[0;32m--> 250\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msingle_step_forward(x_seq[t])\n\u001b[1;32m    251\u001b[0m     y_seq\u001b[39m.\u001b[39mappend(y)\n\u001b[1;32m    252\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstore_v_seq:\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/spikingjelly-0.0.0.0.14-py3.10.egg/spikingjelly/activation_based/neuron.py:907\u001b[0m, in \u001b[0;36mLIFNode.single_step_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingle_step_forward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    906\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m--> 907\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msingle_step_forward(x)\n\u001b[1;32m    908\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    909\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_float_to_tensor(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/spikingjelly-0.0.0.0.14-py3.10.egg/spikingjelly/activation_based/neuron.py:240\u001b[0m, in \u001b[0;36mBaseNode.single_step_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_float_to_tensor(x)\n\u001b[1;32m    239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneuronal_charge(x)\n\u001b[0;32m--> 240\u001b[0m spike \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mneuronal_fire()\n\u001b[1;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneuronal_reset(spike)\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m spike\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/spikingjelly-0.0.0.0.14-py3.10.egg/spikingjelly/activation_based/neuron.py:177\u001b[0m, in \u001b[0;36mBaseNode.neuronal_fire\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mneuronal_fire\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    162\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    * :ref:`API in English <BaseNode.neuronal_fire-en>`\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m    Calculate out spikes of neurons by their current membrane potential and threshold voltage.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msurrogate_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_threshold)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/spikingjelly-0.0.0.0.14-py3.10.egg/spikingjelly/activation_based/surrogate.py:149\u001b[0m, in \u001b[0;36mSurrogateFunctionBase.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspiking:\n\u001b[0;32m--> 149\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspiking_function(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha)\n\u001b[1;32m    150\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprimitive_function(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/spikingjelly-0.0.0.0.14-py3.10.egg/spikingjelly/activation_based/surrogate.py:720\u001b[0m, in \u001b[0;36mATan.spiking_function\u001b[0;34m(x, alpha)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mspiking_function\u001b[39m(x, alpha):\n\u001b[0;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m atan\u001b[39m.\u001b[39;49mapply(x, alpha)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/snn-delays/lib/python3.10/site-packages/spikingjelly-0.0.0.0.14-py3.10.egg/spikingjelly/activation_based/surrogate.py:674\u001b[0m, in \u001b[0;36matan.forward\u001b[0;34m(ctx, x, alpha)\u001b[0m\n\u001b[1;32m    672\u001b[0m     ctx\u001b[39m.\u001b[39msave_for_backward(x)\n\u001b[1;32m    673\u001b[0m     ctx\u001b[39m.\u001b[39malpha \u001b[39m=\u001b[39m alpha\n\u001b[0;32m--> 674\u001b[0m \u001b[39mreturn\u001b[39;00m heaviside(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = SHD_dataloaders(config)\n",
    "model.train_model(train_loader, valid_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [../Datasets/SHD/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/SHD/extract] manually, then SpikingJelly will re-extract files from [../Datasets/SHD/download].\n",
      "The directory [../Datasets/SHD/duration_25] already exists.\n",
      "The directory [../Datasets/SHD/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/SHD/extract] manually, then SpikingJelly will re-extract files from [../Datasets/SHD/download].\n",
      "The directory [../Datasets/SHD/duration_25] already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthvnvtos\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/thanatos/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thanatos/PhD/snn-delays/SNN-delays/wandb/run-20230420_195033-2v89iegl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thvnvtos/Models%20comparison/runs/2v89iegl' target=\"_blank\">(Fine-tune_lr=1.0e-04->1.5e-04_dropout=0.5_lif_SS=False)Testing_code||seed=0||snn_delays||shd||25ms||bins=10</a></strong> to <a href='https://wandb.ai/thvnvtos/Models%20comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thvnvtos/Models%20comparison' target=\"_blank\">https://wandb.ai/thvnvtos/Models%20comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thvnvtos/Models%20comparison/runs/2v89iegl' target=\"_blank\">https://wandb.ai/thvnvtos/Models%20comparison/runs/2v89iegl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====> Epoch 0 : \n",
      "Loss Train = 1.637  |  Best Acc Train = 49.30% \n",
      "Loss Valid = 0.307  |  Best Acc Valid = 93.87%\n",
      "Loss test = 46.05 | acc test = 87.63\n",
      "# Saving best model...\n",
      "=====> Epoch 1 : \n",
      "Loss Train = 1.638  |  Best Acc Train = 49.65% \n",
      "Loss Valid = 0.308  |  Best Acc Valid = 93.93%\n",
      "Loss test = 46.41 | acc test = 87.54\n",
      "=====> Epoch 2 : \n",
      "Loss Train = 1.582  |  Best Acc Train = 50.60% \n",
      "Loss Valid = 0.317  |  Best Acc Valid = 93.72%\n",
      "Loss test = 46.42 | acc test = 87.18\n",
      "=====> Epoch 3 : \n",
      "Loss Train = 1.549  |  Best Acc Train = 51.60% \n",
      "Loss Valid = 0.322  |  Best Acc Valid = 93.71%\n",
      "Loss test = 46.92 | acc test = 87.45\n",
      "=====> Epoch 4 : \n",
      "Loss Train = 1.530  |  Best Acc Train = 51.89% \n",
      "Loss Valid = 0.327  |  Best Acc Valid = 93.34%\n",
      "Loss test = 46.50 | acc test = 87.17\n",
      "=====> Epoch 5 : \n",
      "Loss Train = 1.501  |  Best Acc Train = 53.36% \n",
      "Loss Valid = 0.327  |  Best Acc Valid = 92.88%\n",
      "Loss test = 46.17 | acc test = 87.51\n",
      "=====> Epoch 6 : \n",
      "Loss Train = 1.532  |  Best Acc Train = 52.34% \n",
      "Loss Valid = 0.328  |  Best Acc Valid = 93.22%\n",
      "Loss test = 46.33 | acc test = 87.56\n",
      "=====> Epoch 7 : \n",
      "Loss Train = 1.522  |  Best Acc Train = 52.12% \n",
      "Loss Valid = 0.332  |  Best Acc Valid = 93.12%\n",
      "Loss test = 46.83 | acc test = 87.04\n",
      "=====> Epoch 8 : \n",
      "Loss Train = 1.536  |  Best Acc Train = 51.63% \n",
      "Loss Valid = 0.326  |  Best Acc Valid = 93.10%\n",
      "Loss test = 46.68 | acc test = 87.57\n",
      "=====> Epoch 9 : \n",
      "Loss Train = 1.505  |  Best Acc Train = 52.40% \n",
      "Loss Valid = 0.329  |  Best Acc Valid = 92.88%\n",
      "Loss test = 46.08 | acc test = 87.47\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>acc_test</td><td>█▇▃▆▃▇▇▁▇▆</td></tr><tr><td>acc_train</td><td>▁▂▃▅▅█▆▆▅▆</td></tr><tr><td>acc_valid</td><td>██▇▇▄▁▃▃▂▁</td></tr><tr><td>loss_test</td><td>▁▄▄█▅▂▃▇▆▁</td></tr><tr><td>loss_train</td><td>██▅▃▃▁▃▂▃▁</td></tr><tr><td>loss_valid</td><td>▁▁▄▅▇▇▇█▆▇</td></tr><tr><td>lr_pos</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_w</td><td>▅██▇▅▄▂▁▁▁</td></tr><tr><td>sigma</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tau_m_0</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tau_m_1</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tau_m_2</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tau_s_0</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tau_s_1</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tau_s_2</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>w_0</td><td>▁▂▃▄▆▇████</td></tr><tr><td>w_1</td><td>▁▂▄▅▆▇████</td></tr><tr><td>w_2</td><td>▁▂▃▄▆▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>9</td></tr><tr><td>acc_test</td><td>0.87468</td></tr><tr><td>acc_train</td><td>0.52398</td></tr><tr><td>acc_valid</td><td>0.9288</td></tr><tr><td>loss_test</td><td>0.46082</td></tr><tr><td>loss_train</td><td>1.5052</td></tr><tr><td>loss_valid</td><td>0.32871</td></tr><tr><td>lr_pos</td><td>0.15</td></tr><tr><td>lr_w</td><td>1e-05</td></tr><tr><td>sigma</td><td>1.0</td></tr><tr><td>tau_m_0</td><td>25.0</td></tr><tr><td>tau_m_1</td><td>25.0</td></tr><tr><td>tau_m_2</td><td>25.0</td></tr><tr><td>tau_s_0</td><td>25.0</td></tr><tr><td>tau_s_1</td><td>25.0</td></tr><tr><td>tau_s_2</td><td>0</td></tr><tr><td>w_0</td><td>0.17181</td></tr><tr><td>w_1</td><td>0.16259</td></tr><tr><td>w_2</td><td>0.28052</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">(Fine-tune_lr=1.0e-04->1.5e-04_dropout=0.5_lif_SS=False)Testing_code||seed=0||snn_delays||shd||25ms||bins=10</strong> at: <a href='https://wandb.ai/thvnvtos/Models%20comparison/runs/2v89iegl' target=\"_blank\">https://wandb.ai/thvnvtos/Models%20comparison/runs/2v89iegl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230420_195033-2v89iegl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = SHD_dataloaders(config)\n",
    "model.fine_tune(train_loader, valid_loader, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory [../Datasets/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/extract] manually, then SpikingJelly will re-extract files from [../Datasets/download].\n",
      "The directory [../Datasets/duration_10] already exists.\n",
      "The directory [../Datasets/extract] for saving extracted files already exists.\n",
      "SpikingJelly will not check the data integrity of extracted files.\n",
      "If extracted files are not integrated, please delete [../Datasets/extract] manually, then SpikingJelly will re-extract files from [../Datasets/download].\n",
      "The directory [../Datasets/duration_10] already exists.\n",
      "tensor(-4., device='cuda:0')\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "        -1.7098e-07, -4.7809e-01, -1.7098e-07,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "config.DCLSversion = 'v2'\n",
    "config.model_type = 'snn_delays_lr0'\n",
    "config.wandb_run_name = config.wandb_run_name.replace('(Pre-train)', '(Fine-tune_BIGLR_PLIF)')\n",
    "config.wandb_group_name = config.wandb_group_name.replace('(Pre-train)', '(Fine-tune)')\n",
    "\n",
    "config.lr_w = 1e-3\n",
    "config.max_lr_w = 2 * config.lr_w\n",
    "\n",
    "#config.dropout_p = 0.1\n",
    "#config.stateful_synapse_learnable = True\n",
    "#config.spiking_neuron_type = 'plif'\n",
    "config.epochs = 50\n",
    "\n",
    "train_loader, valid_loader = SHD_dataloaders(config)\n",
    "model_d = SnnDelays(config).to(device)\n",
    "\n",
    "\n",
    "model_d.load_state_dict(torch.load(PATH), strict=False)\n",
    "\n",
    "model_d.round_pos()\n",
    "\n",
    "w = model_d.blocks[0][0][0].weight\n",
    "p = model_d.blocks[0][0][0].P\n",
    "sig = model_d.blocks[0][0][0].SIG\n",
    "\n",
    "kernel = model_d.blocks[0][0][0].GCK.forward(w, p , sig)\n",
    "print(p[0,1,0,0])\n",
    "print(kernel[1,0])\n",
    "\n",
    "print(sig[0,0,0,0])\n",
    "\n",
    "model_d.train_model(train_loader, valid_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7., device='cuda:0')\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -2.5519,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "w = model_d.blocks[0][0][0].weight\n",
    "p = model_d.blocks[0][0][0].P\n",
    "sig = model_d.blocks[0][0][0].SIG\n",
    "\n",
    "kernel = model_d.blocks[0][0][0].GCK.forward(w, p , sig)\n",
    "print(p[0,2,0,0])\n",
    "print(kernel[2,0])\n",
    "\n",
    "print(sig[0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn-delays",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
